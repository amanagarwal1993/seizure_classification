{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import math\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis as ksis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Visualizing one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = loadmat('data/patient_3/test/patient_3_test_1054.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (example['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, channels = ((example['data']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (samples)\n",
    "print (channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel1 = example['data'][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time, channel1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file(filename, feature_extractors):\n",
    "    \"\"\"\n",
    "    Takes in a file path, and a list of functions (feature extractors), that will be used to extract features.\n",
    "    Returns a feature vector with all features for all channels\n",
    "    \"\"\"\n",
    "    # Read data \n",
    "    data = loadmat(filename)\n",
    "    samples, channels = (data['data'].shape)\n",
    "    \n",
    "    all_features = []\n",
    "    \n",
    "    # For each channel\n",
    "    for c in range(channels):\n",
    "        channel = data['data'][:,c]\n",
    "        \n",
    "        # create a list of features\n",
    "        features = []\n",
    "        \n",
    "        # compute each desired feature\n",
    "        for func in feature_extractors:\n",
    "            val = func(channel)\n",
    "            if (math.isnan(val)):\n",
    "                features.append(0)\n",
    "            else:\n",
    "                features.append(val)\n",
    "        \n",
    "        # add this channel's features to the overall feature list of all channels\n",
    "        all_features.append(features)\n",
    "        \n",
    "    all_features = np.array(all_features, dtype=np.float64)\n",
    "        \n",
    "    #for i in range(4):\n",
    "    #    all_features[:,i] = (all_features[:,i] - statistics.mean(all_features[:,i])) / (max(all_features[:,i]) - min(all_features[:,i]))\n",
    "        \n",
    "    return (all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,200))\n",
    "for c in range(channels):\n",
    "    channel = example['data'][:,c]\n",
    "    plt.subplot(48,2,c+1)\n",
    "    plt.plot(time, channel)\n",
    "    plt.title(\"Channel: \" + str(c))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_1053 = (convert_file('data/patient_3/test/patient_3_test_1054.mat', [linelength, variance, energy, kurtosis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (features_1053)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "        features_1053[:,i] = (features_1053[:,i] - statistics.mean(features_1053[:,i])) / (max(features_1053[:,i]) - min(features_1053[:,i]))\n",
    "        \n",
    "print (features_1053)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to convert a file into a feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file_to_sample(filename, feature_extractors):\n",
    "    \"\"\"\n",
    "    Takes in a file path, and a list of functions (feature extractors), that will be used to extract features.\n",
    "    Returns a feature vector with all features for all channels\n",
    "    \"\"\"\n",
    "    # Read data \n",
    "    data = loadmat(filename)\n",
    "    samples, channels = (data['data'].shape)\n",
    "    \n",
    "    all_features = []\n",
    "    \n",
    "    # For each channel\n",
    "    for c in range(channels):\n",
    "        channel = data['data'][:,c]\n",
    "        \n",
    "        # create a list of features\n",
    "        features = []\n",
    "        \n",
    "        # compute each desired feature\n",
    "        for func in feature_extractors:\n",
    "            val = func(channel)\n",
    "            if (np.isnan(val) or math.isnan(val)):\n",
    "                features.append(0)\n",
    "            else:\n",
    "                features.append(val)\n",
    "        \n",
    "        # add this channel's features to the overall feature list of all channels\n",
    "        all_features.append(features)\n",
    "        \n",
    "    all_features = np.array(all_features, dtype=np.float64)\n",
    "        \n",
    "    for i in range(4):\n",
    "        all_features[:,i] = (all_features[:,i] - statistics.mean(all_features[:,i])) / (max(all_features[:,i]) - min(all_features[:,i]))\n",
    "        \n",
    "    return (np.nan_to_num(all_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy(second):\n",
    "    energy = 0\n",
    "    for i in range(second.size):\n",
    "        if not math.isnan(second[i]):\n",
    "            energy += second[i]**2\n",
    "    energy = energy / (second.size)\n",
    "    return energy\n",
    "\n",
    "def variance(second):\n",
    "    return second.var()\n",
    "\n",
    "def linelength(second):\n",
    "    ll = 0\n",
    "    for i in range(second.size):\n",
    "        if not (math.isnan(second[i]) or math.isnan(second[i-1])):\n",
    "            ll += abs(second[i] - second[i-1])\n",
    "    return ll\n",
    "\n",
    "def kurtosis(second):\n",
    "    return ksis(second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Experiments\n",
    "\n",
    "1. Extract features and store in numpy array for all training data points\n",
    "2. Try different classifiers for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ictal files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 424/424 [00:26<00:00, 15.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading non-ictal files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1200/1200 [01:11<00:00, 16.69it/s]\n"
     ]
    }
   ],
   "source": [
    "patient_id = 4\n",
    "\n",
    "patient_str = 'patient_' + str(patient_id)\n",
    "    \n",
    "ictal_files = glob.glob('data/' + patient_str + '/ictal/*.mat')\n",
    "nonictal_files = glob.glob('data/' + patient_str + '/non-ictal/*.mat')\n",
    "#all_files = ictal_files + nonictal_files\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "print (\"Loading ictal files\")\n",
    "for filename in tqdm(ictal_files):\n",
    "    X.append(convert_file_to_sample(filename, [linelength, variance, energy, kurtosis]))\n",
    "    Y.append(1)\n",
    "    \n",
    "print (\"Loading non-ictal files\")\n",
    "for file in tqdm(nonictal_files):\n",
    "    X.append(convert_file_to_sample(filename, [linelength, variance, energy, kurtosis]))\n",
    "    Y.append(0)\n",
    "\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], X.shape[1]*X.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(X, Y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1380, 352)\n",
      "(244, 352)\n"
     ]
    }
   ],
   "source": [
    "print (X_train.shape)\n",
    "print (X_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(random_state=0, tol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=0, tol=1e-05, verbose=0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_dev, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[170,   0],\n",
       "       [  0,  74]], dtype=int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_dev, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_forest_training = forest.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  1.0\n",
      "Area under ROC =  1.0\n",
      "Confusion Matrix:  [[170   0]\n",
      " [  0  74]]\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy = \", forest.score(X_dev, y_dev))\n",
    "print (\"Area under ROC = \", roc_auc_score(y_dev, predictions_forest_training))\n",
    "print (\"Confusion Matrix: \", confusion_matrix(y_dev, predictions_forest_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print (predictions_forest_training[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logistic.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  1.0\n",
      "Area under ROC =  1.0\n",
      "Confusion Matrix:  [[170   0]\n",
      " [  0  74]]\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy = \", logistic.score(X_dev, y_dev))\n",
    "print (\"Area under ROC = \", roc_auc_score(y_dev, predictions))\n",
    "print (\"Confusion Matrix: \", confusion_matrix(y_dev, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/patient_4/test\\\\patient_4_test_1.mat', 'data/patient_4/test\\\\patient_4_test_10.mat', 'data/patient_4/test\\\\patient_4_test_100.mat', 'data/patient_4/test\\\\patient_4_test_1000.mat', 'data/patient_4/test\\\\patient_4_test_1001.mat', 'data/patient_4/test\\\\patient_4_test_1002.mat', 'data/patient_4/test\\\\patient_4_test_1003.mat', 'data/patient_4/test\\\\patient_4_test_1004.mat', 'data/patient_4/test\\\\patient_4_test_1005.mat', 'data/patient_4/test\\\\patient_4_test_1006.mat']\n"
     ]
    }
   ],
   "source": [
    "test_files = glob.glob('data/' + patient_str + '/test/*.mat')\n",
    "print (test_files[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2184"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create feature vector of test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2184/2184 [02:20<00:00, 15.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Empty array\n",
    "test_samples = []\n",
    "\n",
    "# Take in feature vectors to create test dataset\n",
    "for file in tqdm(test_files):\n",
    "    test_samples.append(convert_file_to_sample(file, [linelength, variance, energy, kurtosis]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = test_samples.reshape(test_samples.shape[0], test_samples.shape[1]*test_samples.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run model on test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model on test dataset\n",
    "predictions_svm = clf.predict(test_samples)\n",
    "predictions_forest = forest.predict(test_samples)\n",
    "predictions_logistic = logistic.predict(test_samples)\n",
    "\n",
    "#employee_writer.writerow(['John Smith', 'Accounting', 'November'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(predictions_forest[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/' + patient_str + '/test_scores.csv', mode='w') as test_scores:\n",
    "    writer = csv.writer(test_scores, delimiter=',', lineterminator='\\n')\n",
    "    \n",
    "    writer.writerow(['id', 'prediction'])\n",
    "    \n",
    "    # Write to CSV\n",
    "    for i in range(len(test_files)):\n",
    "        writer.writerow([patient_str+'_'+str(i+1), str(predictions_forest[i])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Training Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "patient_1_ictal = glob.glob('data/patient_1/ictal/*.mat')\n",
    "patient_1_nonictal = glob.glob('data/patient_1/non_ictal/*.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_1 = [1] * len(patient_1_ictal)\n",
    "y_2 = [0] * len(patient_1_nonictal)\n",
    "# x = ...\n",
    "y = y1 + y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def generate_samples(patient_id):\n",
    "    patient_str = 'patient_' + str(patient_id)\n",
    "    \n",
    "    ictal_files = glob.glob('data/' + patient_str + '/ictal/*.mat')\n",
    "    nonictal_files = glob.glob('data/' + patient_str + '/non-ictal/*.mat')\n",
    "    all_files = ictal_files + nonictal_files\n",
    "    \n",
    "    y_1 = [1] * len(ictal_files)\n",
    "    y_2 = [0] * len(nonictal_files)\n",
    "    labels = y_1 + y_2\n",
    "    \n",
    "    X, y = shuffle(all_files, labels)\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        sample = convert_file_to_sample(X[i], [linelength, variance, energy, kurtosis])\n",
    "        label = y[i]\n",
    "        \n",
    "        yield (sample, label)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generator = generate_samples(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "print (generator.__next__())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
